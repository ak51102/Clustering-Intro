{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C4 Clustering Comparisons.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPjNbC3ALCzaG9OdoVLFyOe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/Clustering-Intro/blob/master/C4_Clustering_Comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXPsVNOiYxyz",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction to Clustering: Comparisons**\n",
        "\n",
        "An overview of clustering techniques.\n",
        ">Affinity Propagation<br>\n",
        "Agglomerative Clustering<br>\n",
        "BIRCH<br>\n",
        "DBSCAN<br>\n",
        "K-Means<br>\n",
        "Mini-batch K-Means<br>\n",
        "Mean Shift<br>\n",
        "Gaussian Mixture Model<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lw7hHVwZ01x",
        "colab_type": "text"
      },
      "source": [
        "Each algorithm offers a different approach to the challenge of discovering natural groups in data.\n",
        "\n",
        "There is no best clustering algorithm, and no easy way to find the best algorithm for your data without using controlled experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63PUH_CfxRCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install scikit-learn\n",
        "# check scikit-learn version\n",
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYuCqMqjYbUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7_Kx0aDrekY",
        "colab_type": "text"
      },
      "source": [
        "Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwbS9mZu1I_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_dataset(y, X):\n",
        "  for class_value in range(2):\n",
        "    # get row indexes for samples with this class\n",
        "    row_ix = where(y == class_value)\n",
        "    # create scatter of these samples\n",
        "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAsHM-GmMfu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJhiWFWGlDOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_function(clusters, yhat,X):\n",
        "  for cluster in clusters:\n",
        "    # get row indexes for samples with this cluster\n",
        "    row_ix = where(yhat == cluster)\n",
        "    # create scatter of these samples\n",
        "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KADZWGxIyG3d",
        "colab_type": "text"
      },
      "source": [
        "**Create a Synthetic Dataset**<br>\n",
        "To generate a random n-class classification problem, use the make_classification function.<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0p2wawzswQw",
        "colab_type": "text"
      },
      "source": [
        "The dataset has two distinct clusters. <br>\n",
        "\n",
        "Can the clustering algorithms identifiy these two clusters?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Y0BQKTx3YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# synthetic classification dataset\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, \n",
        "                           n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "# create scatter plot for samples from each class\n",
        "plot_dataset(y, X)\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOOAwx3urE9K",
        "colab_type": "text"
      },
      "source": [
        "**K-Means**<br>\n",
        "K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n",
        "\n",
        "— Some methods for classification and analysis of multivariate observations, 1967."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFe6jBz5rFFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# k-means clustering\n",
        "from sklearn.cluster import KMeans\n",
        "# define the model\n",
        "#KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, \n",
        "#precompute_distances='auto', verbose=0, random_state=None, copy_x=True, \n",
        "#n_jobs=None, algorithm='auto')\n",
        "\n",
        "model = KMeans(n_clusters=2)\n",
        "# fit the model\n",
        "model.fit(X)\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#In this case, a reasonable grouping is found, although the unequal equal \n",
        "#variance in each dimension makes the method less suited to this dataset."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHXev4eHrLnp",
        "colab_type": "text"
      },
      "source": [
        "**Mini-Batch K-Means**<br>\n",
        "Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n",
        "\n",
        "… we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent.\n",
        "\n",
        "— Web-Scale K-Means Clustering, 2010."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUe1WpClrLxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mini-batch k-means clustering\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "# define the model\n",
        "#MiniBatchKMeans(n_clusters=8, init='k-means++', max_iter=100, batch_size=100,\n",
        "#verbose=0, compute_labels=True, random_state=None, tol=0.0, \n",
        "#max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)\n",
        "\n",
        "model = MiniBatchKMeans(n_clusters=2)\n",
        "# fit the model\n",
        "model.fit(X)\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#In this case, a result equivalent to the standard k-means algorithm is found."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_rLqtZAsC4k",
        "colab_type": "text"
      },
      "source": [
        "**Gaussian Mix**<br>\n",
        "A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n",
        "\n",
        "For more on the model, see:\n",
        "\n",
        "[Mixture model, Wikipedia.](https://en.wikipedia.org/wiki/Mixture_model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAa3vonhsDCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gaussian mixture clustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "# define the model\n",
        "#GaussianMixture(n_components=1, covariance_type='full', tol=0.001, \n",
        "#reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', \n",
        "#weights_init=None, means_init=None, precisions_init=None, random_state=None, \n",
        "#warm_start=False, verbose=0, verbose_interval=10)\n",
        "\n",
        "model = GaussianMixture(n_components=2)\n",
        "# fit the model\n",
        "model.fit(X)\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#In this case, we can see that the clusters were identified perfectly. \n",
        "#This is not surprising given that the dataset was generated as a mixture of Gaussians.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlWA8IzlsQxR",
        "colab_type": "text"
      },
      "source": [
        "**Birch**<br>\n",
        "BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using\n",
        "Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n",
        "\n",
        "BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i. e., available memory and time constraints).\n",
        "\n",
        "— BIRCH: An efficient data clustering method for large databases, 1996."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8eq4cyd0h3Z",
        "colab_type": "text"
      },
      "source": [
        "It is implemented via the Birch class and the main configuration to tune is the “threshold” and “n_clusters” hyperparameters, the latter of which provides an estimate of the number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1_oi5wKsQ6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# birch clustering\n",
        "from sklearn.cluster import Birch\n",
        "# define the model\n",
        "#Birch(threshold=0.5, branching_factor=50, n_clusters=2, compute_labels=True, \n",
        "#copy=True)\n",
        "model = Birch(threshold=0.01, n_clusters=2)\n",
        "# fit the model\n",
        "model.fit(X)\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbJI24zgyJ8S",
        "colab_type": "text"
      },
      "source": [
        "**Affinity Propagation**<br>\n",
        "We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges\n",
        "\n",
        "— Clustering by Passing Messages Between Data Points, 2007."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSTcBo5JyKGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# affinity propagation clustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "#1 HYPER PARAMETER TO TUNE 0.5 to 1\n",
        "#AffinityPropagation(damping=0.5, max_iter=200, convergence_iter=15, \n",
        "#copy=True, preference=None, affinity='euclidean', verbose=False)\n",
        "model = AffinityPropagation(damping=0.9, )\n",
        "\n",
        "# fit the model\n",
        "model.fit(X)\n",
        "# assign a cluster to each example\n",
        "yhat = model.predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#Does this alogrithm find the two clusters of the dataset?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNI4P-mnz1Us",
        "colab_type": "text"
      },
      "source": [
        "**Agglomerative Clustering**<br>\n",
        "Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n",
        "\n",
        "It is a part of a broader class of hierarchical clustering methods and you can learn more here:\n",
        "\n",
        "[Hierarchical clustering, Wikipedia](https://en.wikipedia.org/wiki/Hierarchical_clustering)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh4wGxAez1dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# agglomerative clustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "# define the model\n",
        "#AgglomerativeClustering(n_clusters=2, affinity='euclidean', memory=None, \n",
        "#connectivity=None, compute_full_tree='auto', linkage='ward', \n",
        "#distance_threshold=None)\n",
        "model = AgglomerativeClustering(n_clusters=2)\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#In this case, a reasonable grouping is found."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pFXsc-X0JD3",
        "colab_type": "text"
      },
      "source": [
        "**DBSCAN**<br>\n",
        "DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n",
        "\n",
        "… we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it\n",
        "\n",
        "— A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, 1996."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkN0N0YK0H4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dbscan clustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "# define the model\n",
        "#DBSCAN(eps=0.5, min_samples=5, metric='euclidean', metric_params=None, \n",
        "#algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
        "model = DBSCAN(eps=0.3, min_samples=9)\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n",
        "\n",
        "#In this case, a reasonable grouping is found, although more tuning is required."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXVchmkA0u3j",
        "colab_type": "text"
      },
      "source": [
        "**Mean Shift**<br>\n",
        "Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n",
        "\n",
        "We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and thus its utility in detecting the modes of the density.\n",
        "\n",
        "— Mean Shift: A robust approach toward feature space analysis, 2002."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDLjtzRd1Q84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean shift clustering\n",
        "from sklearn.cluster import MeanShift\n",
        "# define the model\n",
        "model = MeanShift()\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTj44MUu1aAY",
        "colab_type": "text"
      },
      "source": [
        "**OPTICS**<br>\n",
        "OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n",
        "\n",
        "We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings.\n",
        "\n",
        "— OPTICS: ordering points to identify the clustering structure, 1999.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12xgG6m81aJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optics clustering\n",
        "from sklearn.cluster import OPTICS\n",
        "# define the model\n",
        "model = OPTICS(eps=0.8, min_samples=10)\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-G6oruQ1i0B",
        "colab_type": "text"
      },
      "source": [
        "**Spectral Clustering**<br>\n",
        "Spectral Clustering is a general class of clustering methods, drawn from linear algebra.\n",
        "\n",
        "A promising alternative that has recently emerged in a number of fields is to use spectral methods for clustering. Here, one uses the top eigenvectors of a matrix derived from the distance between points.\n",
        "\n",
        "— On Spectral Clustering: Analysis and an algorithm, 2002."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn4JoUS21kwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spectral clustering\n",
        "from sklearn.cluster import SpectralClustering\n",
        "# define the model\n",
        "model = SpectralClustering(n_clusters=2)\n",
        "# fit model and predict clusters\n",
        "yhat = model.fit_predict(X)\n",
        "# retrieve unique clusters\n",
        "clusters = unique(yhat)\n",
        "# create scatter plot for samples from each cluster\n",
        "plot_function(clusters, yhat,X)\n",
        "# show the plot\n",
        "pyplot.subplot(1,2,1)\n",
        "plot_function(clusters, yhat,X)\n",
        "#The second subplot\n",
        "pyplot.subplot(1,2,2)\n",
        "plot_dataset(y, X)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4X1ZZ6O4uE7",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/clustering-algorithms-with-python/\n"
      ]
    }
  ]
}